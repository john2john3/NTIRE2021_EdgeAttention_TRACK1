{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tqdm import tqdm\n",
    "\n",
    "for gpu in tf.config.experimental.list_physical_devices(\"GPU\"):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-chambers",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "\n",
    "def scaling(input_image):\n",
    "    input_image = input_image / 255.0\n",
    "    return input_image\n",
    "\n",
    "\n",
    "class Dataloader(Sequence):\n",
    "\n",
    "    def __init__(self, x_set, y_set, batch_size, shuffle = False):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        indices = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        \n",
    "        batch_x = [self.x[i] for i in indices]\n",
    "        batch_y = [self.y[i] for i in indices]\n",
    "        \n",
    "        input_image = np.array([\n",
    "            scaling(img_to_array(load_img(file_name))) for file_name in batch_x])\n",
    "        truth_image = np.array([\n",
    "            scaling(img_to_array(load_img(file_name))) for file_name in batch_y])\n",
    "        \n",
    "        height = input_image.shape[1]\n",
    "        width = input_image.shape[2]\n",
    "    \n",
    "        input_patch_size = 64\n",
    "        scale = 4\n",
    "        \n",
    "        # randomly crop\n",
    "        truth_patch_size = input_patch_size * scale\n",
    "        \n",
    "        input_x = np.random.randint(width - input_patch_size)\n",
    "        input_y = np.random.randint(height - input_patch_size)\n",
    "        truth_x = input_x * scale\n",
    "        truth_y = input_y * scale\n",
    "        \n",
    "        input_patch = input_image[:, input_y:(input_y+input_patch_size), input_x:(input_x+input_patch_size)]\n",
    "        truth_patch = truth_image[:, truth_y:(truth_y+truth_patch_size), truth_x:(truth_x+truth_patch_size)]\n",
    "        \n",
    "        # randomly rotate\n",
    "        rot90_k = np.random.randint(4)+1\n",
    "        input_patch = np.rot90(input_patch, k=rot90_k, axes=(1, 2))\n",
    "        truth_patch = np.rot90(truth_patch, k=rot90_k, axes=(1, 2))\n",
    "        \n",
    "        # randomly flip\n",
    "        flip = (np.random.uniform() < 0.5)\n",
    "        if (flip):\n",
    "            input_patch = input_patch[:, :, ::-1]\n",
    "            truth_patch = truth_patch[:, :, ::-1]\n",
    "            \n",
    "        # finalize\n",
    "        return input_patch, truth_patch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(len(self.x))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "            \n",
    "train_blur_dir = \"./NTIRE/LR/train_blur_bicubic/train/train_blur_bicubic/X4/\"\n",
    "train_blur = []\n",
    "idx=240\n",
    "for folder in tqdm(range(0,idx)):\n",
    "    if folder<=9:\n",
    "        temp = train_blur_dir +(\"00\"+str(folder)+\"/\")\n",
    "    elif folder <=99:\n",
    "        temp = train_blur_dir +(\"0\"+str(folder)+\"/\")\n",
    "    else:\n",
    "        temp = train_blur_dir +(str(folder)+\"/\")\n",
    "    for file in range(0,100):\n",
    "        \n",
    "        if file <=9:\n",
    "            temp_2 = temp + str(\"0000000\")+str(file)+\".png\"\n",
    "                \n",
    "        else:\n",
    "            temp_2 = temp + str(\"000000\")+str(file)+\".png\"\n",
    "        \n",
    "        train_blur.append(temp_2)\n",
    "\n",
    "train_target_dir = \"./NTIRE/LR/train_sharp/train/train_sharp/\"\n",
    "train_target = []\n",
    "for folder in tqdm(range(0,idx)):\n",
    "    if folder<=9:\n",
    "        temp = train_target_dir +(\"00\"+str(folder)+\"/\")\n",
    "    elif folder <=99:\n",
    "        temp = train_target_dir +(\"0\"+str(folder)+\"/\")\n",
    "    else:\n",
    "        temp = train_target_dir +(str(folder)+\"/\")\n",
    "    for file in range(0,100):\n",
    "        if file <=9:\n",
    "            temp_2 = temp + str(\"0000000\")+str(file)+\".png\"\n",
    "                \n",
    "        else:\n",
    "            temp_2 = temp + str(\"000000\")+str(file)+\".png\"\n",
    "        \n",
    "        train_target.append(temp_2)\n",
    "        \n",
    "\n",
    "val_blur_dir = \"./NTIRE/LR/val_blur_bicubic/val/val_blur_bicubic/X4/\"\n",
    "val_blur = []\n",
    "for folder in tqdm(range(0,30)):\n",
    "    if folder%10==9:\n",
    "        if folder<=9:\n",
    "            temp = val_blur_dir +(\"00\"+str(folder)+\"/\")\n",
    "        elif folder <=99:\n",
    "            temp = val_blur_dir +(\"0\"+str(folder)+\"/\")\n",
    "        else:\n",
    "            temp = val_blur_dir +(str(folder)+\"/\")\n",
    "        for file in range(0,100):\n",
    "        \n",
    "            if file <=9:\n",
    "                temp_2 = temp + str(\"0000000\")+str(file)+\".png\"\n",
    "                \n",
    "            else:\n",
    "                temp_2 = temp + str(\"000000\")+str(file)+\".png\"\n",
    "            \n",
    "            val_blur.append(temp_2)\n",
    "\n",
    "val_target_dir = \"./NTIRE/LR/val_sharp/val/val_sharp/\"\n",
    "val_target = []\n",
    "for folder in tqdm(range(0,30)):\n",
    "    if folder%10==9:\n",
    "        if folder<=9:\n",
    "            temp = val_target_dir +(\"00\"+str(folder)+\"/\")\n",
    "        elif folder <=99:\n",
    "            temp = val_target_dir +(\"0\"+str(folder)+\"/\")\n",
    "        else:\n",
    "            temp = val_target_dir +(str(folder)+\"/\")\n",
    "        for file in range(0,100):\n",
    "            \n",
    "            if file <=9:\n",
    "                temp_2 = temp + str(\"0000000\")+str(file)+\".png\"\n",
    "                \n",
    "            else:\n",
    "                temp_2 = temp + str(\"000000\")+str(file)+\".png\"\n",
    "        \n",
    "            val_target.append(temp_2)\n",
    "\n",
    "\n",
    "train_loader = Dataloader(train_blur, train_target, 8, shuffle=True)\n",
    "valid_loader = Dataloader(val_blur, val_target, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(upscale_factor=2, channels=3):\n",
    "    conv_args = {\n",
    "        \"activation\": \"relu\",\n",
    "        \"kernel_initializer\": keras.initializers.glorot_normal(seed=None),\n",
    "        \"padding\": \"same\"\n",
    "    }\n",
    "    conv_args2 = {\n",
    "        \"kernel_initializer\": keras.initializers.glorot_normal(seed=None),\n",
    "        \"padding\": \"same\"\n",
    "    }\n",
    "    \n",
    "    inputs = keras.Input(shape=(None, None, channels))\n",
    "    \n",
    "    \n",
    "    #conv\n",
    "    x1 = layers.Conv2D(filters=128, kernel_size=(9,9),**conv_args)(inputs)\n",
    "    \n",
    "    x = x1\n",
    "    \n",
    "    #block\n",
    "    repetition= 12\n",
    "    for i in range(0,repetition):\n",
    "        r_block = x\n",
    "        x_2 = layers.Conv2D(filters=128, kernel_size=(3,3),**conv_args)(x)\n",
    "        \n",
    "        #attention\n",
    "        sobel = tf.image.sobel_edges(x_2)\n",
    "        sobel = tf.math.abs(sobel[:, :, :, :, 0]) + tf.math.abs(sobel[:, :, :, :, 1])\n",
    "        \n",
    "        sobel = layers.Conv2D(filters=128, kernel_size=(3,3),**conv_args)(sobel)\n",
    "        sobel = layers.Conv2D(filters=128, kernel_size=(3,3),**conv_args2)(sobel)\n",
    "        sobel = keras.activations.sigmoid(sobel)\n",
    "        x = keras.layers.Multiply()([x_2,sobel])\n",
    "        \n",
    "        #atrous conv\n",
    "        ar_1 = layers.Conv2D(filters=64, kernel_size=(3,3),**conv_args,dilation_rate=1)(x_2)\n",
    "        ar_3 = layers.Conv2D(filters=64, kernel_size=(3,3),**conv_args,dilation_rate=3)(x_2)\n",
    "        ar_5 = layers.Conv2D(filters=64, kernel_size=(3,3),**conv_args,dilation_rate=5)(x_2)\n",
    "        ar = tf.concat([ar_1,ar_3,ar_5],axis=-1)\n",
    "        \n",
    "        ar = layers.Conv2D(filters=128, kernel_size=(3,3),**conv_args,dilation_rate=1)(ar)\n",
    "        \n",
    "        ar_1 = layers.Conv2D(filters=64, kernel_size=(3,3),**conv_args,dilation_rate=1)(ar)\n",
    "        ar_3 = layers.Conv2D(filters=64, kernel_size=(3,3),**conv_args,dilation_rate=3)(ar)\n",
    "        ar_5 = layers.Conv2D(filters=64, kernel_size=(3,3),**conv_args,dilation_rate=5)(ar)\n",
    "        ar = tf.concat([ar_1,ar_3,ar_5],axis=-1)\n",
    "        \n",
    "        \n",
    "        #concat attention & atrous\n",
    "        x = tf.concat([x,ar],axis=-1)\n",
    "        \n",
    "        #conv\n",
    "        x = layers.Conv2D(filters=128, kernel_size=(3,3),**conv_args2)(x)\n",
    "        \n",
    "        # residual\n",
    "        x = x + r_block\n",
    "    \n",
    "    # concat\n",
    "    x = tf.concat([x,x1],axis=-1)\n",
    "    \n",
    "    #conv\n",
    "    x = layers.Conv2D(filters=128,kernel_size=(3,3),**conv_args)(x)\n",
    "    \n",
    "    #ESPCN\n",
    "    x = layers.Conv2D(64, 5, **conv_args)(x)\n",
    "    x = layers.Conv2D(64, 3, **conv_args)(x)\n",
    "    x = layers.Conv2D(32, 3, **conv_args)(x)\n",
    "    \n",
    "    x = layers.Conv2D(32 * (upscale_factor ** 2), 3, **conv_args2)(x)\n",
    "    x = tf.nn.depth_to_space(x, upscale_factor)\n",
    "    x = layers.Conv2D(32 * (upscale_factor ** 2), 3, **conv_args2)(x)\n",
    "    x = tf.nn.depth_to_space(x, upscale_factor)\n",
    "    \n",
    "    outputs = layers.Conv2D(3, 3, **conv_args2)(x)\n",
    "    \n",
    "    #skip branch\n",
    "    x_skip = tf.image.resize(inputs,[tf.shape(inputs)[1]*4,tf.shape(inputs)[2]*4],method='bicubic')\n",
    "    \n",
    "    outputs = outputs + x_skip\n",
    "    \n",
    "    \n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-bahamas",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-tourist",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = keras.losses.MeanAbsoluteError()\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-4,\n",
    "    decay_steps=200000,\n",
    "    decay_rate=0.5,\n",
    "    staircase=True)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=lr_schedule,beta_1=0.9, beta_2=0.999, epsilon= 1e-8)\n",
    "epochs = 133\n",
    "model.compile(\n",
    "    optimizer=optimizer, loss=loss_fn\n",
    ")\n",
    "\n",
    "import datetime\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-juice",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "filepath = './save/1/weights.{epoch:02d}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath,             # file명을 지정합니다\n",
    "                             monitor='val_loss',   # val_loss 값이 개선되었을때 호출됩니다\n",
    "                             verbose=1,            # 로그를 출력합니다\n",
    "                             save_best_only=False,  # 가장 best 값만 저장합니다\n",
    "                             save_weights_only=False,\n",
    "                             mode='auto'           # auto는 알아서 best를 찾습니다. min/max\n",
    "                            )\n",
    "model.fit(train_loader, validation_data=valid_loader,batch_size=8, epochs=epochs,callbacks=[checkpoint,tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-ranking",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader(Sequence):\n",
    "\n",
    "    def __init__(self, x_set, y_set, batch_size, shuffle = False):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        indices = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        \n",
    "        batch_x = [self.x[i] for i in indices]\n",
    "        batch_y = [self.y[i] for i in indices]\n",
    "        \n",
    "        input_image = np.array([\n",
    "            scaling(img_to_array(load_img(file_name))) for file_name in batch_x])\n",
    "        truth_image = np.array([\n",
    "            scaling(img_to_array(load_img(file_name))) for file_name in batch_y])\n",
    "        \n",
    "        height = input_image.shape[1]\n",
    "        width = input_image.shape[2]\n",
    "    \n",
    "        input_patch_size = 128\n",
    "        scale = 4\n",
    "        \n",
    "        # randomly crop\n",
    "        truth_patch_size = input_patch_size * scale\n",
    "        \n",
    "        input_x = np.random.randint(width - input_patch_size)\n",
    "        input_y = np.random.randint(height - input_patch_size)\n",
    "        truth_x = input_x * scale\n",
    "        truth_y = input_y * scale\n",
    "        \n",
    "        input_patch = input_image[:, input_y:(input_y+input_patch_size), input_x:(input_x+input_patch_size)]\n",
    "        truth_patch = truth_image[:, truth_y:(truth_y+truth_patch_size), truth_x:(truth_x+truth_patch_size)]\n",
    "        \n",
    "        # randomly rotate\n",
    "        rot90_k = np.random.randint(4)+1\n",
    "        input_patch = np.rot90(input_patch, k=rot90_k, axes=(1, 2))\n",
    "        truth_patch = np.rot90(truth_patch, k=rot90_k, axes=(1, 2))\n",
    "        \n",
    "        # randomly flip\n",
    "        flip = (np.random.uniform() < 0.5)\n",
    "        if (flip):\n",
    "            input_patch = input_patch[:, :, ::-1]\n",
    "            truth_patch = truth_patch[:, :, ::-1]\n",
    "            \n",
    "        # finalize\n",
    "        return input_patch, truth_patch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(len(self.x))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "train_loader = Dataloader(train_blur, train_target, 2, shuffle=True)\n",
    "valid_loader = Dataloader(val_blur, val_target, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-repeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('./save/1/weights.133.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-enzyme",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = keras.losses.MeanAbsoluteError()\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=(1e-4)/4,\n",
    "    decay_steps=200000,\n",
    "    decay_rate=0.5,\n",
    "    staircase=True)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=lr_schedule,beta_1=0.9, beta_2=0.999, epsilon= 1e-8)\n",
    "epochs = 33\n",
    "model.compile(\n",
    "    optimizer=optimizer, loss=loss_fn\n",
    ")\n",
    "\n",
    "import datetime\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-evidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "filepath = './save/2/weights.{epoch:02d}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath,             # file명을 지정합니다\n",
    "                             monitor='val_loss',   # val_loss 값이 개선되었을때 호출됩니다\n",
    "                             verbose=1,            # 로그를 출력합니다\n",
    "                             save_best_only=False,  # 가장 best 값만 저장합니다\n",
    "                             save_weights_only=False,\n",
    "                             mode='auto'           # auto는 알아서 best를 찾습니다. min/max\n",
    "                            )\n",
    "model.fit(train_loader, validation_data=valid_loader,batch_size=2, epochs=epochs,callbacks=[checkpoint,tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-celebration",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader(Sequence):\n",
    "\n",
    "    def __init__(self, x_set, y_set, batch_size, shuffle = False):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        indices = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        \n",
    "        batch_x = [self.x[i] for i in indices]\n",
    "        batch_y = [self.y[i] for i in indices]\n",
    "        \n",
    "        input_image = np.array([\n",
    "            scaling(img_to_array(load_img(file_name))) for file_name in batch_x])\n",
    "        truth_image = np.array([\n",
    "            scaling(img_to_array(load_img(file_name))) for file_name in batch_y])\n",
    "        \n",
    "        height = input_image.shape[1]\n",
    "        width = input_image.shape[2]\n",
    "    \n",
    "        input_patch_size = 180\n",
    "        scale = 4\n",
    "        \n",
    "        # randomly crop\n",
    "        truth_patch_size = input_patch_size * scale\n",
    "        \n",
    "        input_x = np.random.randint(width - input_patch_size)\n",
    "        input_y = 0\n",
    "        truth_x = input_x * scale\n",
    "        truth_y = input_y * scale\n",
    "        \n",
    "        input_patch = input_image[:, input_y:(input_y+input_patch_size), input_x:(input_x+input_patch_size)]\n",
    "        truth_patch = truth_image[:, truth_y:(truth_y+truth_patch_size), truth_x:(truth_x+truth_patch_size)]\n",
    "        \n",
    "        # randomly rotate\n",
    "        rot90_k = np.random.randint(4)+1\n",
    "        input_patch = np.rot90(input_patch, k=rot90_k, axes=(1, 2))\n",
    "        truth_patch = np.rot90(truth_patch, k=rot90_k, axes=(1, 2))\n",
    "        \n",
    "        # randomly flip\n",
    "        flip = (np.random.uniform() < 0.5)\n",
    "        if (flip):\n",
    "            input_patch = input_patch[:, :, ::-1]\n",
    "            truth_patch = truth_patch[:, :, ::-1]\n",
    "            \n",
    "        # finalize\n",
    "        return input_patch, truth_patch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(len(self.x))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "train_loader = Dataloader(train_blur, train_target, 1, shuffle=True)\n",
    "valid_loader = Dataloader(val_blur, val_target, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-skiing",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('./save/2/weights.133.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-forth",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = keras.losses.MeanAbsoluteError()\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=(1e-4)/16,\n",
    "    decay_steps=200000,\n",
    "    decay_rate=0.5,\n",
    "    staircase=True)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=lr_schedule,beta_1=0.9, beta_2=0.999, epsilon= 1e-8)\n",
    "epochs = 11\n",
    "model.compile(\n",
    "    optimizer=optimizer, loss=loss_fn\n",
    ")\n",
    "\n",
    "import datetime\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-gauge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "filepath = './save/3/weights.{epoch:02d}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath,             # file명을 지정합니다\n",
    "                             monitor='val_loss',   # val_loss 값이 개선되었을때 호출됩니다\n",
    "                             verbose=1,            # 로그를 출력합니다\n",
    "                             save_best_only=False,  # 가장 best 값만 저장합니다\n",
    "                             save_weights_only=False,\n",
    "                             mode='auto'           # auto는 알아서 best를 찾습니다. min/max\n",
    "                            )\n",
    "model.fit(train_loader, validation_data=valid_loader,batch_size=1, epochs=epochs,callbacks=[checkpoint,tensorboard_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "han",
   "language": "python",
   "name": "han"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
